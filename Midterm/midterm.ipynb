{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization\n",
    "## Logic Definition of Generalization:\n",
    "1. Show empirically that the information limit of 2 prediction bits per parameter also holds for nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_dimensions  avg_predicted_points  n_full/n_average\n",
      "0             1              0.500000          4.000000\n",
      "1             2              2.625000          1.523810\n",
      "2             3              4.166667          1.920000\n",
      "3             4              8.500000          1.882353\n",
      "4             5             15.450000          2.071197\n",
      "5             6             33.250000          1.924812\n",
      "6             7             63.750000          2.007843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "def n_full(d):\n",
    "    return 2**d\n",
    "\n",
    "def generate_dataset(n_samples, n_dimensions):\n",
    "    \"\"\"Generates a dataset of random points and random labels.\"\"\"\n",
    "    X = np.random.rand(n_samples, n_dimensions)\n",
    "    y = np.random.randint(2, size=n_samples)  # Binary labels\n",
    "    return X, y\n",
    "\n",
    "def train_test_split(X, y, test_size=0.5):\n",
    "    \"\"\"Splits the dataset into training and test sets.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_test = int(n_samples * test_size)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    training_idx, test_idx = indices[n_test:], indices[:n_test]\n",
    "    return X[training_idx], X[test_idx], y[training_idx], y[test_idx]\n",
    "\n",
    "def count_required_points(X, y):\n",
    "    \"\"\"Counts how many points are required to perfectly predict the training set using 1-NN.\"\"\"\n",
    "    # split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    predictions = knn.predict(X_test)\n",
    "    \n",
    "    correct_predictions = (predictions == y_test)\n",
    "    required_points_indices = np.where(correct_predictions == True)[0]\n",
    "    return len(required_points_indices)\n",
    "\n",
    "def experiment(n_dimensions, n_samples):\n",
    "    \"\"\"Conducts the experiment for a given dimensionality and number of functions.\"\"\"\n",
    "    required_points = []\n",
    "    \n",
    "    for _ in range(n_dimensions * 8):\n",
    "        X, y = generate_dataset(n_samples, n_dimensions)\n",
    "        n_required = count_required_points(X, y)\n",
    "        required_points.append(n_required)\n",
    "        \n",
    "    avg_required_points = np.mean(required_points) * 2 # Multiply by 2 because we only used half of the data\n",
    "    return n_dimensions, avg_required_points, n_samples / avg_required_points\n",
    "\n",
    "# Define the dimensions and number of functions for each dimensionality\n",
    "dimensions_functions = [(x, n_full(x)) for x in range(1, 8)]\n",
    "\n",
    "# Conduct the experiment for each dimensionality\n",
    "results = [experiment(d, n) for d, n in dimensions_functions]\n",
    "\n",
    "results = pd.DataFrame(results, columns=[\"n_dimensions\", \"avg_predicted_points\", \"n_full/n_average\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 0.5666666666666667)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import combinations\n",
    "\n",
    "# Generate a synthetic binary dataset\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "n_features = 4\n",
    "X = np.random.randint(2, size=(n_samples, n_features))  # Features\n",
    "y = np.random.randint(2, size=n_samples)  # Binary target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "def generate_if_then_clauses(X, y):\n",
    "    \"\"\"\n",
    "    Generates if-then clauses from the training data and attempts to minimize them.\n",
    "    \"\"\"\n",
    "    clauses = []\n",
    "    for index, row in enumerate(X):\n",
    "        clause = \"IF \"\n",
    "        for i, val in enumerate(row):\n",
    "            clause += f\"feature_{i} == {val} AND \"\n",
    "        clause = clause.rstrip(\" AND \") + f\" THEN outcome == {y[index]}\"\n",
    "        clauses.append(clause)\n",
    "    \n",
    "    # Basic minimization strategy: Deduplication\n",
    "    unique_clauses = list(set(clauses))\n",
    "    \n",
    "    return unique_clauses\n",
    "\n",
    "def evaluate_clauses(clauses, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates the generated clauses on a test set for accuracy.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for test_row in X_test:\n",
    "        prediction = None\n",
    "        for clause in clauses:\n",
    "            condition, outcome = clause.split(\" THEN \")\n",
    "            condition = condition.replace(\"IF \", \"\").split(\" AND \")\n",
    "            if all(f\"feature_{i} == {int(test_row[i])}\" in condition for i in range(len(test_row))):\n",
    "                _, outcome_val = outcome.split(\" == \")\n",
    "                prediction = int(outcome_val)\n",
    "                break\n",
    "        if prediction is None:\n",
    "            prediction = 0  # Default to 0 if no clause matches\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Generate and evaluate clauses\n",
    "clauses = generate_if_then_clauses(X_train, y_train)\n",
    "accuracy = evaluate_clauses(clauses, X_test, y_test)\n",
    "\n",
    "len(clauses), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
